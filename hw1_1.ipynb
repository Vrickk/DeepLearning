{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 딥러닝및실습 HW1_1\n",
        "\n",
        "2018180042 문승벽\n",
        "\n"
      ],
      "metadata": {
        "id": "D9slf6yEITIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 딥러닝과실습 시간에서는 PyTorch를 이용하여 딥러닝의 원리와 이해 그리고 실습을 진행하게 된다.\n",
        "\n",
        "PyTorch를 이해하려면 먼저 Tensor 구조체에 대해 먼저 알고 넘어가야 한다. Tensor 구조체란 하나의 데이터 타입의 원소들을 포함하는 다차원 배열을 의미한다.\n",
        "\n",
        "심층 신경망 (Deep Neural Network)에서는 입력 텐서와 출력 텐서를 어떻게 구성하는지를 배우게 된다.\n",
        "\n",
        "그렇다면 PyTorch 안에서 Tensor는 어떻게 표현되며, 어떤 식으로 사용될까? 예제를 하나씩 실행해 보면서 알아보도록 하겠다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**a. tensor_initialization.py**"
      ],
      "metadata": {
        "id": "3_7ECQ_dK_XB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "LvUBWzfVFtt_",
        "outputId": "c14079d8-9418-4f1a-9333-5c289895b13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "cpu\n",
            "False\n",
            "torch.Size([3])\n",
            "torch.Size([3])\n",
            "################################################## 1\n",
            "torch.int64\n",
            "cpu\n",
            "False\n",
            "torch.Size([3])\n",
            "torch.Size([3])\n",
            "################################################## 2\n",
            "torch.Size([]) 0\n",
            "torch.Size([1]) 1\n",
            "torch.Size([5]) 1\n",
            "torch.Size([5, 1]) 2\n",
            "torch.Size([3, 2]) 2\n",
            "torch.Size([3, 2, 1]) 3\n",
            "torch.Size([3, 1, 2, 1]) 4\n",
            "torch.Size([3, 1, 2, 3]) 4\n",
            "torch.Size([3, 1, 2, 3, 1]) 5\n",
            "torch.Size([4, 5]) 2\n",
            "torch.Size([4, 1, 5]) 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ae7753546b56>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# torch.Tensor class\n",
        "t1 = torch.Tensor([1, 2, 3], device='cpu') # 처음부터 텐서 클래스로 선언. 앞이 대문자로 시작할 경우 클래스 형식의 텐서.\n",
        "print(t1.dtype)   # >>> torch.float32\n",
        "print(t1.device)  # >>> cpu\n",
        "print(t1.requires_grad)  # >>> False\n",
        "print(t1.size())  # torch.Size([3])\n",
        "print(t1.shape)   # torch.Size([3])\n",
        "\n",
        "# if you have gpu device\n",
        "# t1_cuda = t1.to(torch.device('cuda')) # CPU Memory에 있는 값을 GPU Memory로 넘김.\n",
        "# or you can use shorthand\n",
        "# t1_cuda = t1.cuda()\n",
        "\n",
        "t1_cpu = t1.cpu()\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "# torch.tensor function\n",
        "t2 = torch.tensor([1, 2, 3], device='cpu') # 이미 있는 기존 데이터를 '텐서화' 시킴. 텐서 클래스와 기능은 동일하다.\n",
        "print(t2.dtype)  # >>> torch.int64\n",
        "print(t2.device)  # >>> cpu\n",
        "print(t2.requires_grad)  # >>> False\n",
        "print(t2.size())  # torch.Size([3])\n",
        "print(t2.shape)  # torch.Size([3])\n",
        "\n",
        "# if you have gpu device\n",
        "# t2_cuda = t2.to(torch.device('cuda'))\n",
        "# or you can use shorthand\n",
        "# t2_cuda = t2.cuda()\n",
        "t2_cpu = t2.cpu()\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
        "print(a1.shape, a1.ndim)\n",
        "\n",
        "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
        "print(a2.shape, a2.ndim)\n",
        "\n",
        "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
        "print(a3.shape, a3.ndim)\n",
        "\n",
        "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
        "print(a4.shape, a4.ndim)\n",
        "\n",
        "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
        "    [1, 2],\n",
        "    [3, 4],\n",
        "    [5, 6]\n",
        "])\n",
        "print(a5.shape, a5.ndim)\n",
        "\n",
        "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
        "    [[1], [2]],                     # rank는 대괄호의 수와 같다.\n",
        "    [[3], [4]],                     # shape는 한 대괄호 안에 들어있는 원소로, 제일 바깥쪽의 대괄호의 경우부터 따진다.\n",
        "    [[5], [6]]\n",
        "])\n",
        "print(a6.shape, a6.ndim)\n",
        "\n",
        "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
        "    [[[1], [2]]],                   # 대괄호의 수 (rank) = 4\n",
        "    [[[3], [4]]],\n",
        "    [[[5], [6]]]\n",
        "])\n",
        "print(a7.shape, a7.ndim)\n",
        "\n",
        "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
        "    [[[1, 2, 3], [2, 3, 4]]],       # 대괄호의 수 (rank) = 4\n",
        "    [[[3, 1, 1], [4, 4, 5]]],\n",
        "    [[[5, 6, 2], [6, 3, 1]]]\n",
        "])\n",
        "print(a8.shape, a8.ndim)\n",
        "\n",
        "\n",
        "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
        "    [[[[1], [2], [3]], [[2], [3], [4]]]],   # 대괄호의 수 : 5\n",
        "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
        "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
        "])\n",
        "print(a9.shape, a9.ndim)\n",
        "\n",
        "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
        "    [1, 2, 3, 4, 5],                # 대괄호의 수 : 2\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [1, 2, 3, 4, 5],\n",
        "])\n",
        "print(a10.shape, a10.ndim)\n",
        "\n",
        "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
        "    [[1, 2, 3, 4, 5]],              # 대괄호의 수 : 3\n",
        "    [[1, 2, 3, 4, 5]],\n",
        "    [[1, 2, 3, 4, 5]],\n",
        "    [[1, 2, 3, 4, 5]],\n",
        "])\n",
        "print(a10.shape, a10.ndim)\n",
        "\n",
        "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
        "    [[[1, 2, 3], [4, 5]]],          # 배열은 각 데이터 공간의 구조가 정형화되어 있는 특성을 가진다. 이 텐서는 배열이 아닌 마치 리스트와 같이 각 공간의 데이터 길이가 일정하지 않기 때문에 에러를 발생하게 된다.\n",
        "    [[[1, 2, 3], [4, 5]]],          # 한 데이터 공간의 데이터 길이는 3인데 그 뒤의 데이터 공간 길이는 2이다.\n",
        "    [[[1, 2, 3], [4, 5]]],\n",
        "    [[[1, 2, 3], [4, 5]]],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch에서의 텐서는 두 가지 방식으로 생성할 수 있다.\n",
        "\n",
        "**torch.Tensor()**방식은 텐서 클래스를 이용한 방식이다. 처음부터 텐서 형식으로 객체를 선언하고 생성하게 된다. python에서 앞이 대문자일 경우에는 클래스를 의미한다.\n",
        "\n",
        "다른 하나인 **torch.tensor()** 방식은 원래 있던 기존 객체를 텐서**화**하는 것이다.\n",
        "\n",
        "둘의 기능은 거의 동일하다. 표현형의 차이는 있다. Tensor를 이용할 경우 데이터 타입이 float32, tensor를 이용할 경우 데이터 타입이 int64로 지정된다.\n",
        "\n",
        "번외로, t1_cuda = t1.to(torch.device('cuda'))는 GPU를 위한 기능이다.\n",
        "CPU 메모리에 있는 값을 GPU 메모리로 옮기는 역할을 하며, CUDA 설치 시 사용 가능하다.\n",
        "\n",
        "\n",
        "\n",
        "PyTorch에서 텐서를 표현하는 데에는 두 가지 개념이 사용되는데, **shape**와 **ndims(ranks)**가 그것이다.\n",
        "\n",
        "예를 들어 torch.tensor([\n",
        "  [[[1], [2]]], [[[3], [4]]], [[[5], [6]]]\n",
        "  ])라는 형식의 텐서 **a7**이 있다고 하자.\n",
        "  ndims(ranks)는 간단히 말해서 데이터 원소를 감싸고 있는 대괄호 쌍의 개수이다. 해당 텐서에서는 4쌍의 대괄호가 존재하는 것을 확인할 수 있다. 따라서 이 텐서의 ranks는 4라고 할 수 있다.\n",
        "\n",
        "  shape는 각 대괄호 영역 안에 있는 데이터 원소의 개수를 의미한다. 이는 가장 바깥쪽의 대괄호부터 따지게 된다.\n",
        "\n",
        "  해당 텐서의 경우, 가장 바깥쪽 대괄호 안의 데이터 원소는 [[[1], [2]]], [[[3], [4]]], [[[5], [6]]]의 세 개이다.\n",
        "\n",
        "  그 다음 대괄호에는 각각 1개씩의 원소(ex. [[1], [2]])이 들어가 있고, 그 다음에는 2개씩의 원소가 들어가 있다(ex. [1], [2]). 최종적으로, 가장 안쪽의 대괄호에는 1, 2식으로 하나씩의 원소가 들어가 있다.\n",
        "\n",
        "  따라서 이 텐서의 shape는 **torch.Size([3, 1, 2, 1])**의 형태로 출력되게 되는 것이다. 이는 가장 바깥쪽 대괄호부터 안에 3개, 1개, 2개, 1개의 원소를 가지고 있다는 의미이다.\n",
        "\n",
        "  위 예제에서 선언된 텐서 **a11**은 오류를 출력하고 있다. 이는 당연한 결과이다.\n",
        "\n",
        "  해당 에러 메시지를 보면 **ValueError: expected sequence of length 3 at dim 3 (got 2)** 라고 출력되는 것을 확인할 수 있다. 이는 데이터 길이가 3이여야 할 공간에 원소가 2개만 들어가 길이가 2로 끝났다는 의미이다.\n",
        "  \n",
        "  위에서 설명했듯이, 텐서는 하나의 데이터 타입의 원소를 포함하는 다차원 **배열**이다. 배열은 각 데이터 공간의 구조가 정형화되어있어야 한다. 그러한 조건을 어기고 마치 python의 리스트처럼 각 데이터 공간의 길이를 상이하게 설정해 놓았기 때문에 에러가 발생한 것이다. 앞 공간의 원소는 3개이고 뒷 공간의 원소는 2개인 등 한 대괄호 안에서 서로 길이가 상이한 두 공간이 존재하여 에러가 발생한다.\n",
        "\n",
        "  각 공간의 데이터 길이를 모두 같게 해 주면 에러가 발생하지 않는다.\n",
        "\n"
      ],
      "metadata": {
        "id": "StC5QT8MZVnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. tensor_initialization_copy.py**"
      ],
      "metadata": {
        "id": "kmMOALQneqzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "l1 = [1, 2, 3]\n",
        "t1 = torch.Tensor(l1)\n",
        "\n",
        "l2 = [1, 2, 3]\n",
        "t2 = torch.tensor(l2)\n",
        "\n",
        "l3 = [1, 2, 3]\n",
        "t3 = torch.as_tensor(l3) # as_tensor는 기존 데이터를 tensor로 변환시키는 명시적 변환이다.\n",
        "\n",
        "# torch.Tensor와 torch.tensor는 항상 주어진 데이터를 복사한다.\n",
        "l1[0] = 100\n",
        "l2[0] = 100\n",
        "l3[0] = 100\n",
        "\n",
        "print(t1) # tensor([1., 2., 3.]) -> float32\n",
        "print(t2) # tensor([1, 2, 3]) -> int64\n",
        "print(t3) # tensor([1, 2, 3]) -> int64\n",
        "\n",
        "print(\"#\" * 100)\n",
        "\n",
        "l4 = np.array([1, 2, 3])\n",
        "t4 = torch.Tensor(l4)\n",
        "\n",
        "l5 = np.array([1, 2, 3])\n",
        "t5 = torch.tensor(l5)\n",
        "\n",
        "l6 = np.array([1, 2, 3])\n",
        "t6 = torch.as_tensor(l6) # 원본 numpy array에 대해 copy를 막으려면 as_tensor를 사용하면 된다.\n",
        "\n",
        "l4[0] = 100\n",
        "l5[0] = 100\n",
        "l6[0] = 100\n",
        "\n",
        "print(t4) # tensor([1., 2., 3.])\n",
        "print(t5) # tensor([1, 2, 3])\n",
        "print(t6) # tensor([100, 2, 3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLPcVf-We0TX",
        "outputId": "eae6a850-6944-44e4-cd46-532470c08eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3])\n",
            "tensor([1, 2, 3])\n",
            "####################################################################################################\n",
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3])\n",
            "tensor([100,   2,   3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여러가지 방식으로 텐서를 초기화하는 법을 다루고 있다.\n",
        "\n",
        "torch.Tensor()를 사용하는 경우, float32 형식으로 초기화되기 때문에 1., 2., 3.과 같은 형태로 출력되는 것을 확인할 수 있다.\n",
        "\n",
        "torch.tensor()를 이용할 경우 int64 형식으로 초기화되기 때문에 1, 2, 3과 같은 형태로 출력된다.\n",
        "\n",
        "torch.as_tensor()는 기존 데이터를 명시적으로 텐서로 변환하는 기능이다. 마찬가지로 int64 형식으로 초기화된다.\n",
        "\n",
        "torch.Tensor와 torch.tensor는 항상 주어진 데이터를 복사한다.\n",
        "\n",
        "numpy array 역시 위의 세 방식을 이용해 텐서로 변환이 가능한데, 이 때 copy를 피하고 싶다면 as_tensor를 이용하면 된다. 예제의 t6이 출력된 것을 보면 as_tensor를 이용하여 텐서를 초기화한 이후에도 첫 번째 원소를 변경할 수 있는 것을 알 수 있다."
      ],
      "metadata": {
        "id": "Mqi2tFTOf5Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. tensor_initialization_constant_values.py**"
      ],
      "metadata": {
        "id": "bvYDoRpVhjer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.ones(size=(5,))  # or torch.ones(5) 원소가 모두 1인 크기 5의 벡터\n",
        "t1_like = torch.ones_like(input=t1)\n",
        "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
        "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
        "\n",
        "t2 = torch.zeros(size=(6,))  # or torch.zeros(6) 원소가 모두 0인 크기 6의 벡터\n",
        "t2_like = torch.zeros_like(input=t2) # 이 때 input은 없어도 됨\n",
        "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
        "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
        "\n",
        "t3 = torch.empty(size=(4,))  # or torch.zeros(4) 초기화되지 않은 데이터를 원소로 하는 크기 4의 벡터 생성\n",
        "t3_like = torch.empty_like(input=t3)\n",
        "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
        "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
        "\n",
        "# ~_like = 입력 텐서(input)와 동일한 shape의 텐서를 생성. 원소는 앞에 붙는 내용에 따라 달라짐.\n",
        "\n",
        "t4 = torch.eye(n=3) # 크기 3x3의 항등행렬 생성\n",
        "print(t4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVV39REahpv_",
        "outputId": "161feaa9-6590-40ed-aa33-541c4ce70a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "tensor([1., 1., 1., 1., 1.])\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "tensor([-5.4777e-11,  3.2691e-41, -5.4685e-11,  3.2691e-41])\n",
            "tensor([-4.4914e-35,  4.3269e-41, -4.4914e-35,  4.3269e-41])\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.ones는 모든 원소가 1인 벡터를 생성한다.\n",
        "\n",
        "torch.zeros는 모든 원소가 0인 벡터를 생성한다.\n",
        "\n",
        "torch.empty는 모든 원소가 초기화되지 않은 쓰레기값인 벡터를 생성한다. t3의 출력값을 보면 초기화가 안된 쓰레기값들이 마구잡이로 출력되는 것을 볼 수 있다.\n",
        "\n",
        "이 셋은 size에 지정된 값대로 벡터의 크기가 정해진다. 또한 뒤에 _like를 붙임으로써 다른 텐서의 모형을 빌려 그 원소를 0, 1, 쓰레기값으로 채울 수 있다.\n",
        "\n",
        "예를 들어, ones_like는 input의 텐서 shape와 동일한 shape의 텐서를 생성하는데, 이 때 원소를 전부 1로 채워서 생성한다.\n",
        "\n",
        "torch.eyes(n)는 항등행렬을 생성한다. 이 때 항등행렬의 크기는 n x n이다. n이 3인 경우엔 3x3크기의 항등행렬을 생성한다.\n"
      ],
      "metadata": {
        "id": "XebOHyMmif_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**d. tensor_initialization_random_values.py**"
      ],
      "metadata": {
        "id": "U9OYikijktte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# size에 지정된 shape대로 텐서를 만들고 텐서를 랜덤한 값으로 채우는 메서드들\n",
        "\n",
        "t1 = torch.randint(low=10, high=20, size=(1, 2)) # 최소값과 최대값 사이의 랜덤한 수들을 채워 생성.\n",
        "print(t1)\n",
        "\n",
        "t2 = torch.rand(size=(1, 3))  # 연속균등분포를 이용하여 0과 1 사이 랜덤한 float 값으로 채워진 텐서\n",
        "print(t2)\n",
        "\n",
        "t3 = torch.randn(size=(1, 3)) # 표준정규분포 X~N(0,1)로부터 float값을 생성해서 채움.\n",
        "print(t3)\n",
        "\n",
        "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2)) # mean과 standard deviation이 주어진 정규분포를 이용하여 랜덤한 float값을 생성해서 채움\n",
        "print(t4)\n",
        "\n",
        "t5 = torch.linspace(start=0.0, end=5.0, steps=3) # 시작부터 끝까지 간격이 동일한 1차원 텐서를 반환\n",
        "print(t5)\n",
        "\n",
        "t6 = torch.arange(5) # 시작부터 끝까지 step만큼 간격이 있는 1차원 텐서 반환. start와 step은 사전 지정되어 있고 보통 end를 입력하여 사용한다.\n",
        "print(t6)\n",
        "\n",
        "print(\"#\" * 30)\n",
        "\n",
        "torch.manual_seed(1729) # 고정된 랜덤수를 생성할 수 있도록 시드 설정.\n",
        "random1 = torch.rand(2, 3)\n",
        "print(random1)\n",
        "\n",
        "random2 = torch.rand(2, 3)\n",
        "print(random2)\n",
        "\n",
        "print()\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random3 = torch.rand(2, 3)\n",
        "print(random3)\n",
        "\n",
        "random4 = torch.rand(2, 3)\n",
        "print(random4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbn3j4dSk5go",
        "outputId": "e221fac3-3ea7-48f1-ad45-a48b11cc5f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[17, 12]])\n",
            "tensor([[0.0453, 0.5035, 0.9978]])\n",
            "tensor([[0.7419, 0.5923, 0.2908]])\n",
            "tensor([[ 9.2270, 10.0961],\n",
            "        [ 9.4067,  9.5878],\n",
            "        [10.0763, 11.1161]])\n",
            "tensor([0.0000, 2.5000, 5.0000])\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "##############################\n",
            "tensor([[0.3126, 0.3791, 0.3087],\n",
            "        [0.0736, 0.4216, 0.0691]])\n",
            "tensor([[0.2332, 0.4047, 0.2162],\n",
            "        [0.9927, 0.4128, 0.5938]])\n",
            "\n",
            "tensor([[0.3126, 0.3791, 0.3087],\n",
            "        [0.0736, 0.4216, 0.0691]])\n",
            "tensor([[0.2332, 0.4047, 0.2162],\n",
            "        [0.9927, 0.4128, 0.5938]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본적으로 size argument가 주어지고, size에 지정한 shape대로 텐서를 생성한 후, 그 원소들을 특정 랜덤값으로 채우는 메서드들이다.\n",
        "\n",
        "torch.randint(low=0, high, size,...)는 최소값과 최대값 사이의 랜덤한 int값들을 채워 생성한다.\n",
        "\n",
        "torch.rand(size, ...)는 연속균등분포를 이용하여 0과 1 사이 랜덤한 float 값으로 채워진 텐서를 반환한다.\n",
        "\n",
        "torch.randn(size, ...)는 표준정규분포 X~N(0,1)로부터 float값을 생성해서 채워 반환한다.\n",
        "\n",
        "torch.normal(mean, std, size, ...)은 mean과 standard deviation이 주어진 정규분포를 이용하여 랜덤한 float값을 생성해서 채워 반환한다.\n",
        "\n",
        "torch.linspace(start, end, steps, ...)와 torch.arange(start=0, end, steop=1,...)는 size가 주어지지 않는다. 대신 1차원 텐서만을 반환한다.\n",
        "\n",
        "torch.linspace는 시작과 끝을 지정하고, 그 시작부터 끝까지 간격이 동일한 1차원 텐서를 반환한다. torch.arange는 시작이 0, 간격이 1로 이미 지정되어 있다. end를 지정하면, 그 지정된 end까지 1씩 간격이 주어진 1차원 텐서를 반환한다.\n",
        "\n",
        "torch.manual_seed(seed)는 시드값을 지정하여 랜덤수를 고정한다. 시드를 통해 만들어진 결과는 reproducible하다.\n"
      ],
      "metadata": {
        "id": "OI1OpzUfnw4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e. tensor_type_conversion.py**"
      ],
      "metadata": {
        "id": "4MnSkz5qpzOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.ones((2, 3))\n",
        "print(a.dtype) # float32\n",
        "\n",
        "b = torch.ones((2, 3), dtype=torch.int16)\n",
        "print(b) # tensor([[1,1,1], [1,1,1]], dtype=int16)\n",
        "\n",
        "c = torch.rand((2, 3), dtype=torch.float64) * 20. # * - broadcast. 뒤의 스칼라 값이 앞 텐서에 다 들어가게 됨.\n",
        "print(c) # tensor([[f., f., f.], [f., f., f.]], dtype=float64)\n",
        "\n",
        "d = b.to(torch.int32) # dtype = int32\n",
        "print(d)\n",
        "\n",
        "double_d = torch.ones(10, 2, dtype=torch.double)\n",
        "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
        "\n",
        "double_d = torch.zeros(10, 2).double()\n",
        "short_e = torch.ones(10, 2).short()\n",
        "\n",
        "double_d = torch.zeros(10, 2).to(torch.double)\n",
        "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
        "\n",
        "double_d = torch.zeros(10, 2).type(torch.double)\n",
        "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
        "\n",
        "print(double_d.dtype) # float64\n",
        "print(short_e.dtype) # int16\n",
        "\n",
        "double_f = torch.rand(5, dtype=torch.double)\n",
        "short_g = double_f.to(torch.short)\n",
        "print((double_f * short_g).dtype) # float64. float와 short 중에서는 short가 더 크기 때문이다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQaZjVh7p7kx",
        "outputId": "c7f6810f-bb21-40ac-e10a-179515bdab25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1]], dtype=torch.int16)\n",
            "tensor([[18.0429,  7.2532, 19.6519],\n",
            "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1]], dtype=torch.int32)\n",
            "torch.float64\n",
            "torch.int16\n",
            "torch.float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서 타입 변환을 다루고 있다. dtype을 따로 지정해 주면 원하는 타입대로 텐서를 생성할 수 있다.\n",
        "\n",
        "9번 라인의 *는 broadcast 기능을 한다.\n",
        " broadcast할 시 * 뒤의 스칼라 값에 대한 연산이 앞의 텐서의 각 element에 다 적용된다.\n",
        "\n",
        " 한 텐서를 다른 형식의 텐서와 연산할 때, 최종적으로는 제일 작은 데이터 형식으로 남게 된다. 예를 들어 32번 라인의 수식의 결과 텐서는 float64(double)의 형식을 가지고 있는데, 이는 int16(short)보다 float64가 더 작기 때문이다.\n"
      ],
      "metadata": {
        "id": "O1CZJ0MlsYPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**f. tensor_operations.py**"
      ],
      "metadata": {
        "id": "N6x5bIOGuL4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 텐서끼리의 모든 연산은 element-wise -> element끼리 연산됨.\n",
        "\n",
        "t1 = torch.ones(size=(2, 3))\n",
        "t2 = torch.ones(size=(2, 3))\n",
        "t3 = torch.add(t1, t2) # 덧셈\n",
        "t4 = t1 + t2\n",
        "print(t3)\n",
        "print(t4)\n",
        "\n",
        "print(\"#\" * 30)\n",
        "\n",
        "t5 = torch.sub(t1, t2) # 뺄셈\n",
        "t6 = t1 - t2\n",
        "print(t5)\n",
        "print(t6)\n",
        "\n",
        "print(\"#\" * 30)\n",
        "\n",
        "t7 = torch.mul(t1, t2) # 곱셈. broadcasting적용.\n",
        "t8 = t1 * t2\n",
        "print(t7)\n",
        "print(t8)\n",
        "\n",
        "print(\"#\" * 30)\n",
        "\n",
        "t9 = torch.div(t1, t2) # 나눗셈\n",
        "t10 = t1 / t2\n",
        "print(t9)\n",
        "print(t10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFam-AcQuZ0B",
        "outputId": "0354fb55-2570-4ba0-85d4-95db3f9d2008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "##############################\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "##############################\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "##############################\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서의 연산 메서드들이다. 이 메서드들은 element-wise한 성격을 가지고 있다. element-wise란 각 element끼리 계산되는 성질을 말한다. 예를 들어 [[1, 2], [3, 4]]와 [[5, 6], [7, 8]]이 있고 이를 더한다고 하면 각 element끼리 더하여 [[6, 8], [10, 12]]가 되는 것이다.\n",
        "\n",
        "곱셈과 나눗셈도 마찬가지이다. 결과 텐서에는 각 element들끼리 서로 곱하거나 나뉜 결과가 저장된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Id6XlrScvv_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**g. tensor_operations_mm.py**"
      ],
      "metadata": {
        "id": "-Uvg2Jq8whKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.dot( # 1차원 텐서 간의 행렬곱연산\n",
        "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
        ")\n",
        "print(t1, t1.size())\n",
        "\n",
        "t2 = torch.randn(2, 3)\n",
        "t3 = torch.randn(3, 2)\n",
        "t4 = torch.mm(t2, t3) # matrix multiplication - broadcasting 없이 행렬곱 수행\n",
        "print(t4, t4.size())\n",
        "\n",
        "t5 = torch.randn(10, 3, 4)\n",
        "t6 = torch.randn(10, 4, 5)\n",
        "t7 = torch.bmm(t5, t6) # batch matrix multiplication - broadcasting 없이 배치 행렬곱 수행.\n",
        "print(t7.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAZjImljwf67",
        "outputId": "62fe63ca-85d0-49ec-f213-3b22b6ae8eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7) torch.Size([])\n",
            "tensor([[1.6750, 2.2840],\n",
            "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
            "torch.Size([10, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.dot은 두 1차원 텐서들의 행렬곱연산을 수행한다. 각 텐서의 element들을 각각 곱한 값을 더한 결과만을 포함한 텐서를 반환한다.\n",
        "\n",
        "torch.mm은 matrix multiplication의 준말로 broadcasting 없이 2차원 행렬끼리의 곱셈 연산을 한다. (2D Tensor, nxm)과 (2D Tensor, mxp)간의 연산이라면 (nxp) 텐서가 결과로 나오는 식이다.\n",
        "\n",
        "torch.bmm은 batch matrix multiplication의 준말로 3차원 행렬의 곱연산만 지원한다. (b x n x m) 과 (b x m x p)의 연산이라면 (b x n x p) 텐서를 결과로 반환한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "9j5JMRX5xlLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**h. tensor_operations_matmul.py**"
      ],
      "metadata": {
        "id": "TggNqAQUzkQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# matmul : input 텐서 shape에 따라 다양한 모드 지원. broadcasting 지원.\n",
        "\n",
        "# vector x vector: dot product\n",
        "t1 = torch.randn(3)\n",
        "t2 = torch.randn(3)\n",
        "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
        "\n",
        "# matrix x vector: broadcasted dot\n",
        "t3 = torch.randn(3, 4)\n",
        "t4 = torch.randn(4)\n",
        "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
        "\n",
        "# batched matrix x vector: broadcasted dot\n",
        "t5 = torch.randn(10, 3, 4)\n",
        "t6 = torch.randn(4)\n",
        "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
        "\n",
        "# batched matrix x batched matrix: bmm\n",
        "t7 = torch.randn(10, 3, 4)\n",
        "t8 = torch.randn(10, 4, 5)\n",
        "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
        "\n",
        "# batched matrix x matrix: bmm\n",
        "t9 = torch.randn(10, 3, 4)\n",
        "t10 = torch.randn(4, 5)\n",
        "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDsT7J1LzxXQ",
        "outputId": "436308d5-fc32-4d05-ac3c-104872265d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([])\n",
            "torch.Size([3])\n",
            "torch.Size([10, 3])\n",
            "torch.Size([10, 3, 5])\n",
            "torch.Size([10, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "matmul에 대해 다루고 있다. matmul은 input 텐서의 shape에 따라 각기 다른 연산 모드를 제공한다.\n",
        "\n",
        "input이 vector(1차원 텐서) x vector일 경우 matmul은 dot product 연산을 수행한다.\n",
        "\n",
        "matrix(2차원 텐서) x vector일 경우 broadcast된 dot 연산을 수행한다. batched matrix(3차원 텐서) x vector일 경우에도 마찬가지이다. matrix일 경우에는 size가 matrix 사이즈의 0번째 값을 따라가고, batched matrix일 경우에는 batched matrix 사이즈의 0, 1번째 값을 따라간다.\n",
        "\n",
        "batched matrix x batched matrix인 경우 bmm연산을 수행한다.\n",
        "\n",
        "batched matrix x matrix인 경우에도 bmm 연산을 수행한다."
      ],
      "metadata": {
        "id": "hC5GohRpziwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**i. tensor_broadcasting.py**"
      ],
      "metadata": {
        "id": "rIk_Yeum3018"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# 크기가 더 작은 텐서가 더 큰 텐서로 broadcast 된다.\n",
        "\n",
        "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
        "t2 = 2.0\n",
        "print(t1 * t2)\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
        "t4 = torch.tensor([4, 5])\n",
        "print(t3 - t4)\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
        "print(t5 + 2.0)  # t5.add(2.0)\n",
        "print(t5 - 2.0)  # t5.sub(2.0)\n",
        "print(t5 * 2.0)  # t5.mul(2.0)\n",
        "print(t5 / 2.0)  # t5.div(2.0)\n",
        "# 텐서 각 element에 2.0에 대한 연산이 적용.\n",
        "\n",
        "print(\"#\" * 50, 3)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "  return x / 255\n",
        "\n",
        "\n",
        "t6 = torch.randn(3, 28, 28)\n",
        "print(normalize(t6).size())\n",
        "\n",
        "print(\"#\" * 50, 4)\n",
        "\n",
        "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
        "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
        "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
        "t10 = torch.tensor([7])  # torch.Size([1])\n",
        "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
        "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
        "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
        "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
        "\n",
        "# 각 텐서는 적어도 한 차원은 가지고 있어야 한다. 빈 텐서로는 broadcast가 성립하지 않는다.\n",
        "# 각 차원이 같거나, 적어도 하나는 크기가 1이여야 한다.\n",
        "\n",
        "print(\"#\" * 50, 5)\n",
        "\n",
        "t11 = torch.ones(4, 3, 2)\n",
        "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
        "print(t12.shape)\n",
        "\n",
        "t13 = torch.ones(4, 3, 2)\n",
        "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
        "print(t14.shape)\n",
        "\n",
        "t15 = torch.ones(4, 3, 2)\n",
        "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
        "print(t16.shape)\n",
        "\n",
        "t17 = torch.ones(5, 3, 4, 1)\n",
        "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
        "print((t17 + t18).size())\n",
        "\n",
        "# 두 텐서의 차원 크기를 비교할 때, 마지막부터 시작해서 처음까지 비교한다.\n",
        "\n",
        "print(\"#\" * 50, 6)\n",
        "\n",
        "t19 = torch.empty(5, 1, 4, 1)\n",
        "t20 = torch.empty(3, 1, 1)\n",
        "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
        "\n",
        "t21 = torch.empty(1)\n",
        "t22 = torch.empty(3, 1, 7)\n",
        "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
        "\n",
        "t23 = torch.ones(3, 3, 3)\n",
        "t24 = torch.ones(3, 1, 3)\n",
        "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
        "\n",
        "# t25 = torch.empty(5, 2, 4, 1)\n",
        "# t26 = torch.empty(3, 1, 1)\n",
        "# print((t25 + t26).size())\n",
        "# RuntimeError: The size of tensor a (2) must match\n",
        "# the size of tensor b (3) at non-singleton dimension 1\n",
        "\n",
        "# 두 텐서의 차원 크기를 비교할 때, 마지막부터 시작해서 처음까지 비교한다.\n",
        "\n",
        "print(\"#\" * 50, 7)\n",
        "\n",
        "t27 = torch.ones(4) * 5\n",
        "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
        "\n",
        "t28 = torch.pow(t27, 2)\n",
        "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
        "\n",
        "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
        "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
        "t29 = torch.pow(a, exp)\n",
        "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n",
        "# 승 연산도 broadcast가 된다. 또한 pow도 element-wise하게 연산된다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGHkuaPr3_-b",
        "outputId": "4bd34eb7-f44d-4ded-c4ea-1e5b6aa24654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 4., 6.])\n",
            "################################################## 1\n",
            "tensor([[-4, -4],\n",
            "        [-2, -1],\n",
            "        [ 6,  5]])\n",
            "################################################## 2\n",
            "tensor([[3., 4.],\n",
            "        [5., 6.]])\n",
            "tensor([[-1.,  0.],\n",
            "        [ 1.,  2.]])\n",
            "tensor([[2., 4.],\n",
            "        [6., 8.]])\n",
            "tensor([[0.5000, 1.0000],\n",
            "        [1.5000, 2.0000]])\n",
            "################################################## 3\n",
            "torch.Size([3, 28, 28])\n",
            "################################################## 4\n",
            "tensor([[4, 3],\n",
            "        [3, 4]])\n",
            "tensor([[6, 7],\n",
            "        [2, 5]])\n",
            "tensor([[8, 6],\n",
            "        [5, 3]])\n",
            "tensor([[ 8,  9],\n",
            "        [ 7, 10]])\n",
            "################################################## 5\n",
            "torch.Size([4, 3, 2])\n",
            "torch.Size([4, 3, 2])\n",
            "torch.Size([4, 3, 2])\n",
            "torch.Size([5, 3, 4, 1])\n",
            "################################################## 6\n",
            "torch.Size([5, 3, 4, 1])\n",
            "torch.Size([3, 1, 7])\n",
            "torch.Size([3, 3, 3])\n",
            "################################################## 7\n",
            "tensor([5., 5., 5., 5.])\n",
            "tensor([25., 25., 25., 25.])\n",
            "tensor([  1.,   4.,  27., 256.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "broadcast는 두 텐서가 있을 때, 크기가 작은 텐서 값이 더 큰 텐서의 각 element에 연산되는 형태로 이루어진다. 예를 들어, [[2], [1]] * 2.0이라면 [[4.0], [2.0]]의 형태가 되는 것이다.\n",
        "\n",
        "broadcast에서 더 작은 배열이 발견되면, 더 큰 배열의 크기만큼 가상으로 값이 copy되어 연산이 수행되게 된다.\n",
        "\n",
        "broadcast 시 각 텐서는 최소 하나의 차원을 가지고 있어야 하며, 빈 텐서가 있어서는 안 된다. 두 텐서의 차원 크기를 비교할 때는 마지막부터 처음까지 비교한다. 각 차원은 같아야 하며, 최소 하나의 차원이 1이거나, 차원이 적어도 하나의 텐서에서 존재하지 않아야 한다.\n",
        "\n",
        "승 연산 (pow)도 broadcast가 가능하며, element-wise하게 연산된다.\n"
      ],
      "metadata": {
        "id": "EA3olmX88E8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**j. tensor_indexing_slicing.py**\n"
      ],
      "metadata": {
        "id": "7vy3DIo-5VT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(\n",
        "  [[0, 1, 2, 3, 4],\n",
        "   [5, 6, 7, 8, 9],\n",
        "   [10, 11, 12, 13, 14]]\n",
        ")\n",
        "\n",
        "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
        "print(x[:, 1])  # >>> tensor([1, 6, 11]) 각 대괄호의 1번째 원소만 출력\n",
        "print(x[1, 2])  # >>> tensor(7) 1번째 대괄호의 2번째 원소 출력\n",
        "print(x[:, -1])  # >>> tensor([4, 9, 14) 각 대괄호의 마지막 원소 출력\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]]) 1번째 대괄호부터 끝까지 출력\n",
        "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]]) 1번째 대괄호부터 각 대괄호의 3번째 원소부터 끝까지 출력\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "y = torch.zeros((6, 6))\n",
        "y[1:4, 2] = 1 # 1번째부터 4번째 대괄호의 2번째 원소를 1로 변경\n",
        "print(y)\n",
        "\n",
        "print(y[1:4, 1:4])\n",
        "\n",
        "print(\"#\" * 50, 3)\n",
        "\n",
        "z = torch.tensor(\n",
        "  [[1, 2, 3, 4],\n",
        "   [2, 3, 4, 5],\n",
        "   [5, 6, 7, 8]]\n",
        ")\n",
        "print(z[:2]) # 뒤에서부터 대괄호 2개만 출력\n",
        "print(z[1:, 1:3]) # 첫번째 대괄호부터 끝까지, 각각 1번째부터 3번째 원소만 출력\n",
        "print(z[:, 1:]) # 처음부터 끝까지, 각각 1번째부터 끝까지 원소 출력\n",
        "\n",
        "z[1:, 1:3] = 0\n",
        "print(z) # 첫번째 대괄호부터 각각 1번째부터 3번째 원소를 전부 0으로 변경"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xula3FH25dNE",
        "outputId": "7f90ef78-c53d-42f6-82dc-3dfe2e9ce43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 6, 7, 8, 9])\n",
            "tensor([ 1,  6, 11])\n",
            "tensor(7)\n",
            "tensor([ 4,  9, 14])\n",
            "################################################## 1\n",
            "tensor([[ 5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14]])\n",
            "tensor([[ 8,  9],\n",
            "        [13, 14]])\n",
            "################################################## 2\n",
            "tensor([[0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0., 1., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 1., 0.]])\n",
            "################################################## 3\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [2, 3, 4, 5]])\n",
            "tensor([[3, 4],\n",
            "        [6, 7]])\n",
            "tensor([[2, 3, 4],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [2, 0, 0, 5],\n",
            "        [5, 0, 0, 8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서의 슬라이싱과 인덱싱을 다루고 있다. 텐서의 슬라이싱 및 인덱싱은 python 리스트의 그것과 크게 다른 점은 없다.\n",
        "\n",
        "텐서 x가 있다고 할 때 x[1:3, 1:3]하는 식으로 텐서의 슬라이싱 및 인덱싱을 수행할 수 있다. 콤마 앞은 몇 번째 대괄호를, 혹은 슬라이싱으로 어디부터 어디까지를 선택할 것인지, 콤마 뒤쪽은 선택한 대괄호의 몇 번째 원소를 선택할 것인지를 나타낸다.\n",
        "\n",
        "이를 이용하여 특정한 원소만 선택해서 변경하거나 변경할 범위를 정할 수 있다."
      ],
      "metadata": {
        "id": "RpL-jzK-HgQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k. tensor_reshaping.py**"
      ],
      "metadata": {
        "id": "jl97J9wc5j0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "t2 = t1.view(3, 2)  # Shape becomes (3, 2) 데이터 조작 없이 텐서의 shape를 변경. 반환된 텐서는 원본 텐서의 데이터를 받는다.\n",
        "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6) reshape는 non-contiguous한 값을 반환할 수도 있다.\n",
        "print(t2)\n",
        "print(t3)\n",
        "\n",
        "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
        "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
        "print(t4)\n",
        "print(t5)\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "# Original tensor with shape (1, 3, 1)\n",
        "t6 = torch.tensor([[[1], [2], [3]]])\n",
        "\n",
        "# Remove all dimensions of size 1\n",
        "t7 = t6.squeeze()  # Shape becomes (3,) # 크기가 1인 차원을 전부 지우거나 크기가 1인 지정된 차원을 지운다.\n",
        "\n",
        "# Remove dimension at position 0\n",
        "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
        "print(t7)\n",
        "print(t8)\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "# Original tensor with shape (3,)\n",
        "t9 = torch.tensor([1, 2, 3])\n",
        "\n",
        "# Add a new dimension at position 1\n",
        "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1) # 텐서의 특정 위치에 새로운 차원을 추가.\n",
        "print(t10)\n",
        "\n",
        "t11 = torch.tensor(\n",
        "  [[1, 2, 3],\n",
        "   [4, 5, 6]]\n",
        ")\n",
        "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
        "print(t12, t12.shape)\n",
        "\n",
        "print(\"#\" * 50, 3)\n",
        "\n",
        "# Original tensor with shape (2, 3)\n",
        "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Flatten the tensor\n",
        "t14 = t13.flatten()  # Shape becomes (6,) 다차원 텐서를 1차원 텐서로 만든다. start_dim과 end_dim이 주어졌을 경우 해당 조건을 만족하는 차원만 flatten한다.\n",
        "\n",
        "print(t14)\n",
        "\n",
        "# Original tensor with shape (2, 2, 2)\n",
        "t15 = torch.tensor([[[1, 2],\n",
        "                     [3, 4]],\n",
        "                    [[5, 6],\n",
        "                     [7, 8]]])\n",
        "t16 = torch.flatten(t15)\n",
        "\n",
        "t17 = torch.flatten(t15, start_dim=1)\n",
        "\n",
        "print(t16)\n",
        "print(t17)\n",
        "\n",
        "print(\"#\" * 50, 4)\n",
        "\n",
        "t18 = torch.randn(2, 3, 5)\n",
        "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
        "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3]) # 원하는 모든 원소나 차원을 서로 맞바꿔 모양을 바꿀 수 있다.\n",
        "\n",
        "# Original tensor with shape (2, 3)\n",
        "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Permute the dimensions\n",
        "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
        "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
        "print(t20)\n",
        "print(t21)\n",
        "\n",
        "# Transpose the tensor 특별히 두 개의 차원만 바꿀 수 있다.\n",
        "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
        "\n",
        "print(t22)\n",
        "\n",
        "t23 = torch.t(t19)  # Shape becomes (3, 2) 해당 텐서의 0번째와 1번째 차원을 맞바꾼다.\n",
        "\n",
        "print(t23)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mdwh1zP5rjH",
        "outputId": "83254659-2a06-4989-ef6b-0ff7ad12ca92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "tensor([[1, 2, 3, 4, 5, 6]])\n",
            "tensor([[0, 1, 2, 3],\n",
            "        [4, 5, 6, 7]])\n",
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "################################################## 1\n",
            "tensor([1, 2, 3])\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "################################################## 2\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "tensor([[[1, 2, 3]],\n",
            "\n",
            "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
            "################################################## 3\n",
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "################################################## 4\n",
            "torch.Size([2, 3, 5])\n",
            "torch.Size([5, 2, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서의 모양을 조작할 수 있는 메서드들이다.\n",
        "\n",
        "torch.view는 데이터를 조작하지 않고도 텐서의 shape를 입력값대로 조작할 수 있다. 이렇게 반환된 텐서는 원본 텐서와 같은 데이터를 가지고 있다.\n",
        "\n",
        "torch.reshape도 비슷한 기능을 하지만, torch.reshape를 사용할 경우 non-contiguous한 텐서의 카피를 반환할 수 있다.\n",
        "\n",
        "torch.unsqueeze는 텐서의 특정 부분에 새로운 차원을 추가한다. torch.squeeze는 크기가 1인 모든 차원을 없애거나, 특별히 지정한 크기가 1인 차원만을 없앨 수 있다.\n",
        "\n",
        "torch.flatten은 다차원 텐서를 통합하여 1차원 텐서로 만든다. start_dim과 end_dim이 특별히 주어졌다면, 해당 start_dim으로 시작하고 end_dim으로 끝나는 범위 내의 차원만 flatten한다.\n",
        "\n",
        "torch.permute는 원하는 모든 원소와 차원을 서로 맞바꿔 텐서 구조를 변경할 수 있다. torch.transpose는 특별히 두 차원만 지정하여 서로 바꿀 수 있다. torch.t의 경우에는 2차원 텐서만 받는다고 상정하고, 해당 텐서의 0번째와 1번째 차원을 맞바꾼다."
      ],
      "metadata": {
        "id": "3Q1L13WhKpMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**l. tensor_concat.py**"
      ],
      "metadata": {
        "id": "cPMnvlTf5gCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.zeros([2, 1, 3])\n",
        "t2 = torch.zeros([2, 3, 3])\n",
        "t3 = torch.zeros([2, 2, 3])\n",
        "\n",
        "t4 = torch.cat([t1, t2, t3], dim=1) # 텐서의 특정 차원을 병합한다.\n",
        "print(t4.shape)\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
        "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
        "\n",
        "t7 = torch.cat((t5, t6), dim=0)\n",
        "print(t7.shape)  # >>> torch.Size([8])\n",
        "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7]) # 텐서의 0번째 차원을 병합했다. 두 벡터의 모든 원소가 하나의 벡터에 통합되어 들어가 있다.\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
        "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
        "\n",
        "# 2차원 텐서간 병합\n",
        "t10 = torch.cat((t8, t9), dim=0)\n",
        "print(t10.size())  # >>> torch.Size([4, 3])\n",
        "print(t10)\n",
        "# >>> tensor([[ 0,  1,  2],\n",
        "#             [ 3,  4,  5],\n",
        "#             [ 6,  7,  8],\n",
        "#             [ 9, 10, 11]])\n",
        "\n",
        "t11 = torch.cat((t8, t9), dim=1)\n",
        "print(t11.size())  # >>>torch.Size([2, 6])\n",
        "print(t11)\n",
        "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
        "#             [ 3,  4,  5,  9, 10, 11]])\n",
        "\n",
        "print(\"#\" * 50, 3)\n",
        "\n",
        "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
        "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
        "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
        "\n",
        "t15 = torch.cat((t12, t13, t14), dim=0)\n",
        "print(t15.size())  # >>> torch.Size([6, 3])\n",
        "print(t15)\n",
        "# >>> tensor([[ 0,  1,  2],\n",
        "#             [ 3,  4,  5],\n",
        "#             [ 6,  7,  8],\n",
        "#             [ 9, 10, 11],\n",
        "#             [12, 13, 14],\n",
        "#             [15, 16, 17]])\n",
        "\n",
        "t16 = torch.cat((t12, t13, t14), dim=1)\n",
        "print(t16.size())  # >>> torch.Size([2, 9])\n",
        "print(t16)\n",
        "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
        "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
        "\n",
        "print(\"#\" * 50, 4)\n",
        "\n",
        "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
        "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
        "\n",
        "t19 = torch.cat((t17, t18), dim=0)\n",
        "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
        "print(t19)\n",
        "# >>> tensor([[[ 0,  1,  2],\n",
        "#              [ 3,  4,  5]],\n",
        "#             [[ 6,  7,  8],\n",
        "#              [ 9, 10, 11]]])\n",
        "\n",
        "t20 = torch.cat((t17, t18), dim=1)\n",
        "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
        "print(t20)\n",
        "# >>> tensor([[[ 0,  1,  2],\n",
        "#              [ 3,  4,  5],\n",
        "#              [ 6,  7,  8],\n",
        "#              [ 9, 10, 11]]])\n",
        "\n",
        "t21 = torch.cat((t17, t18), dim=2)\n",
        "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
        "print(t21)\n",
        "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
        "#              [ 3,  4,  5,  9, 10, 11]]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLWoU-tI538K",
        "outputId": "6363f70a-5799-464e-aa03-b19390dd3af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "################################################## 1\n",
            "torch.Size([8])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
            "################################################## 2\n",
            "torch.Size([4, 3])\n",
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]])\n",
            "torch.Size([2, 6])\n",
            "tensor([[ 0,  1,  2,  6,  7,  8],\n",
            "        [ 3,  4,  5,  9, 10, 11]])\n",
            "################################################## 3\n",
            "torch.Size([6, 3])\n",
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11],\n",
            "        [12, 13, 14],\n",
            "        [15, 16, 17]])\n",
            "torch.Size([2, 9])\n",
            "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
            "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
            "################################################## 4\n",
            "torch.Size([2, 2, 3])\n",
            "tensor([[[ 0,  1,  2],\n",
            "         [ 3,  4,  5]],\n",
            "\n",
            "        [[ 6,  7,  8],\n",
            "         [ 9, 10, 11]]])\n",
            "torch.Size([1, 4, 3])\n",
            "tensor([[[ 0,  1,  2],\n",
            "         [ 3,  4,  5],\n",
            "         [ 6,  7,  8],\n",
            "         [ 9, 10, 11]]])\n",
            "torch.Size([1, 2, 6])\n",
            "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
            "         [ 3,  4,  5,  9, 10, 11]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.cat은 두 텐서를 병합한다. dim에 특정한 값이 주어졌을 경우, 그 값에 해당하는 차원만 병합한다. 예를 들어 dim=1이라면 각 1번째 차원만 병합한다.\n",
        "\n",
        "병합의 대상이 되는 차원만 건드리고 나머지 차원은 변경하지 않은 채 그대로 유지한다."
      ],
      "metadata": {
        "id": "5ADU183DNW-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hXKlycdxN8yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**m. tensor_stacking.py**"
      ],
      "metadata": {
        "id": "PDurT8V55-zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
        "\n",
        "t3 = torch.stack([t1, t2], dim=0) # 텐서의 시퀀스를 새로운 차원으로 확장하여 병합한다.\n",
        "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
        "print(t3.shape, t3.equal(t4)) # torch.Size([2, 2, 3]) True\n",
        "\n",
        "t5 = torch.stack([t1, t2], dim=1)\n",
        "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
        "print(t5.shape, t5.equal(t6)) # torch.Size([2, 2, 3]) True\n",
        "\n",
        "t7 = torch.stack([t1, t2], dim=2)\n",
        "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
        "print(t7.shape, t7.equal(t8)) # torch.Size([2, 3, 2]) True\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
        "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
        "\n",
        "print(t9.size(), t10.size())\n",
        "# >>> torch.Size([3]) torch.Size([3])\n",
        "\n",
        "t11 = torch.stack((t9, t10), dim=0)\n",
        "print(t11.size())  # >>> torch.Size([2,3])\n",
        "print(t11)\n",
        "# >>> tensor([[0, 1, 2],\n",
        "#             [3, 4, 5]])\n",
        "\n",
        "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
        "print(t11.equal(t12))\n",
        "# >>> True\n",
        "\n",
        "t13 = torch.stack((t9, t10), dim=1)\n",
        "print(t13.size())  # >>> torch.Size([3,2])\n",
        "print(t13)\n",
        "# >>> tensor([[0, 3],\n",
        "#             [1, 4],\n",
        "#             [2, 5]])\n",
        "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
        "print(t13.equal(t14))\n",
        "# >>> True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9Xn9ST36FzH",
        "outputId": "b85e50ce-7ffb-4e21-d9b5-12676f3f5ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 3]) True\n",
            "torch.Size([2, 2, 3]) True\n",
            "torch.Size([2, 3, 2]) True\n",
            "################################################## 1\n",
            "torch.Size([3]) torch.Size([3])\n",
            "torch.Size([2, 3])\n",
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "True\n",
            "torch.Size([3, 2])\n",
            "tensor([[0, 3],\n",
            "        [1, 4],\n",
            "        [2, 5]])\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.stack은 새로운 차원으로 확장하여 텐서들의 시퀀스를 병합한다. torch.unsqueeze를 이용하여 특정 위치에 새로운 차원을 만들어내는 것을 생각하면 된다. 이를 통해 여러 텐서들을 말 그대로 '쌓아' 나간다.\n",
        "\n",
        "dim=0일 때는 0번째 위치에 크기가 1인 새로운 차원을 만들고, dim=1일 때는 1번째 위치에 크기가 1인 새로운 차원을 만드는 식으로 차원을 확장하여 여러 텐서들을 하나로 병합한다."
      ],
      "metadata": {
        "id": "5t5rksH_Ojf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**n. tensor_vstack_hstack.py**"
      ],
      "metadata": {
        "id": "dp7DHrGM6LlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([1, 2, 3])\n",
        "t2 = torch.tensor([4, 5, 6])\n",
        "t3 = torch.vstack((t1, t2)) # 텐서를 수직으로 쌓아 나간다.\n",
        "print(t3)\n",
        "# >>> tensor([[1, 2, 3],\n",
        "#             [4, 5, 6]])\n",
        "\n",
        "t4 = torch.tensor([[1], [2], [3]])\n",
        "t5 = torch.tensor([[4], [5], [6]])\n",
        "t6 = torch.vstack((t4, t5))\n",
        "# >>> tensor([[1],\n",
        "#             [2],\n",
        "#             [3],\n",
        "#             [4],\n",
        "#             [5],\n",
        "#             [6]])\n",
        "\n",
        "t7 = torch.tensor([\n",
        "  [[1, 2, 3], [4, 5, 6]],\n",
        "  [[7, 8, 9], [10, 11, 12]]\n",
        "])\n",
        "print(t7.shape)\n",
        "# >>> (2, 2, 3)\n",
        "\n",
        "t8 = torch.tensor([\n",
        "  [[13, 14, 15], [16, 17, 18]],\n",
        "  [[19, 20, 21], [22, 23, 24]]\n",
        "])\n",
        "print(t8.shape)\n",
        "# >>> (2, 2, 3)\n",
        "\n",
        "t9 = torch.vstack([t7, t8])\n",
        "print(t9.shape)\n",
        "# >>> (4, 2, 3)\n",
        "\n",
        "print(t9)\n",
        "# >>> tensor([[[ 1,  2,  3],\n",
        "#              [ 4,  5,  6]],\n",
        "#             [[ 7,  8,  9],\n",
        "#              [10, 11, 12]],\n",
        "#             [[13, 14, 15],\n",
        "#              [16, 17, 18]],\n",
        "#             [[19, 20, 21],\n",
        "#              [22, 23, 24]]])\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "t10 = torch.tensor([1, 2, 3])\n",
        "t11 = torch.tensor([4, 5, 6])\n",
        "t12 = torch.hstack((t10, t11))\n",
        "print(t12)\n",
        "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "t13 = torch.tensor([[1], [2], [3]])\n",
        "t14 = torch.tensor([[4], [5], [6]])\n",
        "t15 = torch.hstack((t13, t14)) # 텐서를 수평으로 쌓아 나간다.\n",
        "print(t15)\n",
        "# >>> tensor([[1, 4],\n",
        "#             [2, 5],\n",
        "#             [3, 6]])\n",
        "\n",
        "t16 = torch.tensor([\n",
        "  [[1, 2, 3], [4, 5, 6]],\n",
        "  [[7, 8, 9], [10, 11, 12]]\n",
        "])\n",
        "print(t16.shape)\n",
        "# >>> (2, 2, 3)\n",
        "\n",
        "t17 = torch.tensor([\n",
        "  [[13, 14, 15], [16, 17, 18]],\n",
        "  [[19, 20, 21], [22, 23, 24]]\n",
        "])\n",
        "print(t17.shape)\n",
        "# >>> (2, 2, 3)\n",
        "\n",
        "t18 = torch.hstack([t16, t17])\n",
        "print(t18.shape)\n",
        "# >>> (2, 4, 3)\n",
        "\n",
        "print(t18)\n",
        "# >>> tensor([[[ 1,  2,  3],\n",
        "#              [ 4,  5,  6],\n",
        "#              [13, 14, 15],\n",
        "#              [16, 17, 18]],\n",
        "#             [[ 7,  8,  9],\n",
        "#              [10, 11, 12],\n",
        "#              [19, 20, 21],\n",
        "#              [22, 23, 24]]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSmUZib86QTb",
        "outputId": "c0bbba76-578a-4fc4-aebb-5168a31d6f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 2, 3])\n",
            "torch.Size([2, 2, 3])\n",
            "torch.Size([4, 2, 3])\n",
            "tensor([[[ 1,  2,  3],\n",
            "         [ 4,  5,  6]],\n",
            "\n",
            "        [[ 7,  8,  9],\n",
            "         [10, 11, 12]],\n",
            "\n",
            "        [[13, 14, 15],\n",
            "         [16, 17, 18]],\n",
            "\n",
            "        [[19, 20, 21],\n",
            "         [22, 23, 24]]])\n",
            "################################################## 1\n",
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "torch.Size([2, 2, 3])\n",
            "torch.Size([2, 2, 3])\n",
            "torch.Size([2, 4, 3])\n",
            "tensor([[[ 1,  2,  3],\n",
            "         [ 4,  5,  6],\n",
            "         [13, 14, 15],\n",
            "         [16, 17, 18]],\n",
            "\n",
            "        [[ 7,  8,  9],\n",
            "         [10, 11, 12],\n",
            "         [19, 20, 21],\n",
            "         [22, 23, 24]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vstack은 vertical stack의 준말로, 수직으로 텐서를 쌓아 나간다는 뜻이다. 텐서들은 모두 같은 column을 유지해야 한다.\n",
        "\n",
        "hstack은 horizontal stack의 준말로, 수평으로 텐서를 쌓아 나간다는 뜻이다. 텐서들은 모두 같은 row를 유지해야 한다."
      ],
      "metadata": {
        "id": "_qaPHKBaPliw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 숙제 후기\n",
        "\n",
        "솔직히 처음 배웠을 때는 텐서의 구조 정도까지만 이해할 수 있었지만 이렇게 직접 예제를 확인해보고 수업 내용을 토대로 하나하나 분석해보니 어떠한 메서드를 이용하여 텐서를 더 효율적으로 이용할 수 있을지 감이 잡히기 시작한 것 같다. 약간 공감각적인 능력을 요하는 개념인지라 처음에는 조금 혼란스러웠지만 이렇게 각 메서드의 사용법과 사용 예시를 하나하나씩 학습해 보니 어떤 느낌인지 이해가 되는 것 같다."
      ],
      "metadata": {
        "id": "4M51bxdzQNED"
      }
    }
  ]
}